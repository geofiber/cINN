{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "from models import DeepConditionalModel, InvariantNetwork\n",
    "from losses import maximum_likelihood_loss\n",
    "from inn_utils import train_loop_active\n",
    "from viz import plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SummaryNetwork(tf.keras.Model):\n",
    "    \"\"\"A model to preprocess the conditional input via 1x1 convolution.\"\"\"\n",
    "    \n",
    "    def __init__(self, summary_dim=256):\n",
    "        \"\"\"Creates a GRU network to learn summary statistics.\"\"\"\n",
    "        \n",
    "        super(SummaryNetwork, self).__init__()\n",
    "        \n",
    "        self.gru = tf.keras.layers.CuDNNGRU(summary_dim)\n",
    "        \n",
    "    def call(self, x, **kwargs):\n",
    "        \n",
    "        return self.gru(x)\n",
    "    \n",
    "class InvariantModule(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, h_dim):\n",
    "        \"\"\"Creates an invariant function with mean pooling.\"\"\"\n",
    "        \n",
    "        super(InvariantModule, self).__init__()\n",
    "        \n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(h_dim, activation='elu'),\n",
    "            tf.keras.layers.Dense(h_dim, activation='elu'),\n",
    "            tf.keras.layers.Dense(h_dim, activation='elu')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.module(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EquivariantModule(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, h_dim):\n",
    "        \"\"\"Creates an invariant function with mean pooling.\"\"\"\n",
    "        \n",
    "        super(EquivariantModule, self).__init__()\n",
    "        \n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(h_dim, activation='elu'),\n",
    "            tf.keras.layers.Dense(h_dim, activation='elu'),\n",
    "            tf.keras.layers.Dense(h_dim, activation='elu')\n",
    "        ])\n",
    "        \n",
    "        self.invariant_module = InvariantModule(h_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        x_inv = self.invariant_module(x)\n",
    "        x_inv = tf.stack([x_inv] * int(x.shape[1]), axis=1)\n",
    "        x = tf.concat((x_inv, x), axis=-1)\n",
    "        return self.module(x)\n",
    "        \n",
    "    \n",
    "class PermutationInvariantNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, h_dim):\n",
    "        \"\"\"Creates a GRU network to learn summary statistics.\"\"\"\n",
    "        \n",
    "        super(PermutationInvariantNetwork, self).__init__()\n",
    "        \n",
    "        self.equiv1 = EquivariantModule(h_dim)\n",
    "        self.equiv2 = EquivariantModule(h_dim)\n",
    "        self.inv = InvariantModule(h_dim)\n",
    "        \n",
    "    def call(self, x, **kwargs):\n",
    "        \n",
    "        x = self.equiv1(x)\n",
    "        x = self.equiv2(x)\n",
    "        x = self.inv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression class\n",
    "\n",
    "<p>In this example, we train a cINN to approximate the posterior distribution of the regression weights of a Bayesian linear regression model.</p>\n",
    "<hr>\n",
    "<p><strong>Setting:</strong></p>\n",
    "<p>Assume we have observed $n$ pairs of data points:</p>\n",
    "$$D = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1,...,n}$$\n",
    "<p>where</p>\n",
    "$$\\mathbf{x} \\in \\mathbb{R}^d, y \\in \\mathbb{R}$$\n",
    "<p><strong>Model:</strong></p>\n",
    "<p>We model each $y_{i}$ as being drawn from a Gaussian distribution paramaterized by:</p>\n",
    "$$y_{i} \\sim \\mathcal{N}(\\mathbf{\\beta}^T\\mathbf{x}, a^{-1})$$\n",
    "<p>where $a$ is the precision, (i.e., inverse variance), so $a = \\dfrac{1}{\\sigma_{y}^2}$</p>\n",
    "<p>We place the followng prior on the regression weights:</p>\n",
    "$$\\beta \\sim \\mathcal{N}_{d}(\\mathbf{0}, b^{-1}\\mathbf{I}_{d})$$\n",
    "<p>where $\\mathcal{N}_{d}(., .)$ denotes a $d$-dimensional Gaussian distribtuion</p> and $b$ is the precision of the distribution, i.e. $b = \\dfrac{1}{\\sigma_{\\beta}^2}$.\n",
    "<p>When $a$ and $b$ are known, the likelihood $p(D|\\mathbf{\\beta})$ is proportional to:</p>\n",
    "$$p(D|\\mathbf{\\beta}) \\propto \\exp(-\\dfrac{a}{2}(y - \\mathbf{X}\\mathbf{\\beta})^T(y - \\mathbf{X}\\mathbf{\\beta}))$$\n",
    "<p>and the posterior of $\\mathbf{\\beta}$ is proportional to:</p>\n",
    "$$p(\\mathbf{\\beta}|D) \\propto \\exp(-\\dfrac{a}{2}(y - \\mathbf{X}\\mathbf{\\beta})^T(y - \\mathbf{X}\\mathbf{\\beta}) - \\dfrac{b}{2}\\mathbf{\\beta}^T\\mathbf{\\beta})$$\n",
    "<p>where $X$ is the design matrix in which each row $i$ represents $\\mathbf{x}_{i}^T$. Since the prior is conjugate to the likelihood, the posterior is also Gaussian with the following form:</p>\n",
    "$$p(\\mathbf{\\beta}|D) = \\mathcal{N}_{d}(\\mathbf{\\beta}|\\mathbf{\\mu}, \\mathbf{\\Lambda}^{-1})$$\n",
    "<p>where $\\mathbf{\\Lambda}$ is the precision matrix (inverse covariance matrix) and the posterior mean and covariance are calculated as follows:</p>\n",
    "<br>\n",
    "$$\\mathbf{\\Lambda} = a\\mathbf{X}^T\\mathbf{X} + b\\mathbf{I}_{d}$$\n",
    "$$\\mathbf{\\mu} = a\\mathbf{\\Lambda}^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "<p>The preceding calculation are implemented in the class <em>BayesianLinearRegression</em> defined below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     9,
     12,
     16,
     24,
     42
    ]
   },
   "outputs": [],
   "source": [
    "class BayesianLinearRegression:\n",
    "    \n",
    "    def __init__(self, theta_dim, b=1., a=1.):\n",
    "        self.theta_dim = theta_dim\n",
    "        self.b = b # Precision of p(w|b) ~ N(0, b^-1I)\n",
    "        self.a = a # Precision of p(y | beta.Tx, a^-1)\n",
    "        self.Ix = np.identity(self.theta_dim)\n",
    "        self.prior_beta = stats.multivariate_normal(np.zeros(self.theta_dim), (1/self.b) * self.Ix)\n",
    "        \n",
    "    def _generate_design_mat(self, n_samples):\n",
    "        return np.random.randn(n_samples, self.theta_dim)\n",
    "    \n",
    "    def _sample_beta(self):\n",
    "        \"\"\"Prior on beta is a normal N(0, b^-1I).\"\"\"\n",
    "        return self.prior_beta.rvs()\n",
    "    \n",
    "    def generate_data(self, n_samples):\n",
    "        \"\"\"Generates a single regression dataset.\"\"\"\n",
    "        \n",
    "        X = self._generate_design_mat(n_samples)\n",
    "        beta = self._sample_beta()\n",
    "        y = X @ beta + np.random.normal(0., np.sqrt((1/self.a)), n_samples)\n",
    "        return np.c_[X, y], beta\n",
    "    \n",
    "    def generate_multiple_datasets(self, n_datasets, to_tensor=True, n=None):\n",
    "        \"\"\"Generates multiple regression datasets by calling generate_data() multiple times.\"\"\"\n",
    "        \n",
    "        # Draw n from U(50, 500), if n not fixed\n",
    "        if n is None:\n",
    "            n = np.random.randint(50, 501)\n",
    "        \n",
    "        X = np.zeros((n_datasets, n, self.theta_dim+1))\n",
    "        betas = np.zeros((n_datasets, self.theta_dim))\n",
    "        \n",
    "        for i in range(n_datasets):\n",
    "            D, beta = self.generate_data(n)\n",
    "            X[i] = D\n",
    "            betas[i] = beta\n",
    "        if to_tensor:\n",
    "            return tf.convert_to_tensor(X, dtype=tf.float32), tf.convert_to_tensor(betas, dtype=tf.float32)\n",
    "        return X, betas\n",
    "    \n",
    "    def posterior_beta(self, D, n_samples=None):\n",
    "        \"\"\"Computes the posterior of beta given data as described above.\"\"\"\n",
    "        \n",
    "        # Extract X and y\n",
    "        X = D[:, :self.theta_dim]\n",
    "        y = D[:, self.theta_dim]\n",
    "        \n",
    "        # Compute mean and cov of Gaussian posterior\n",
    "        cov = np.linalg.inv(self.a * X.T @ X + self.b * self.Ix)\n",
    "        mu = self.a * cov @ X.T @ y\n",
    "        \n",
    "        if n_samples is None:\n",
    "            return stats.multivariate_normal(mu, cov)\n",
    "        return stats.multivariate_normal(mu, cov).rvs(n_samples)\n",
    "    \n",
    "    def __call__(self, n_datasets, to_tensor=True, n=None):\n",
    "        return self.generate_multiple_datasets(n_datasets, to_tensor=True, n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     15,
     58,
     122
    ]
   },
   "outputs": [],
   "source": [
    "def plot_true_est_posterior(model, reg, n_samples, X_test, Y_test, params_names, \n",
    "                            figsize=(15, 20), filename=None):\n",
    "    \"\"\"Plots X_test.shape[0] rows of posterior samples vs true means.\"\"\"\n",
    "    \n",
    "    # Initialize figure\n",
    "    n_sim = X_test.numpy().shape[0]\n",
    "    f, axarr = plt.subplots(n_sim, len(params_names), figsize=figsize)\n",
    "    \n",
    "    \n",
    "    # Sample from approximate posterior given current Y\n",
    "    if model is not None:\n",
    "        X_samples = model.sample(Y_test, n_samples).numpy()\n",
    "        X_samples_means = X_samples.mean(axis=0)\n",
    "    \n",
    "    # For each row \n",
    "    for i in range(n_sim):\n",
    "        \n",
    "        # Sample from true posterior given current Y\n",
    "        X_analytic_samples = reg.posterior_beta(Y_test[i].numpy(), n_samples=n_samples)\n",
    "        X_analytic_means = X_analytic_samples.mean(axis=0)\n",
    "        \n",
    "        # Extracttrue data generating values\n",
    "        X_true = X_test.numpy()[i]\n",
    "        \n",
    "        for j in range(len(params_names)):\n",
    "            \n",
    "            # Plot KDE of approximate posterior, of model specified\n",
    "            if model is not None:\n",
    "                sns.distplot(X_samples[:, i, j], kde=True, hist=True, ax=axarr[i, j], \n",
    "                             label='Estimated', color='#5c92e8')\n",
    "            # Plot KDE of analytic posterior\n",
    "            sns.distplot(X_analytic_samples[:, j], kde=True, hist=True, ax=axarr[i, j], \n",
    "                         label='Analytic', color='#e55e5e')\n",
    "            \n",
    "            # Plot lines for approximate mean, analytic mean and true data-generating value\n",
    "            if model is not None:\n",
    "                axarr[i, j].axvline(X_samples_means[i, j], color='#5c92e8')\n",
    "            axarr[i, j].axvline(X_analytic_means[j], color='#e55e5e')\n",
    "            axarr[i, j].axvline(X_true[j], color='black', label='True')\n",
    "            axarr[i, j].spines['right'].set_visible(False)\n",
    "            axarr[i, j].spines['top'].set_visible(False)\n",
    "            \n",
    "            # Set title of first row\n",
    "            if i == 0:\n",
    "                axarr[i, j].set_title(params_names[j])\n",
    "            \n",
    "            if i == 0 and j == 0:\n",
    "                axarr[i, j].legend(fontsize=10)\n",
    "            \n",
    "    f.tight_layout()\n",
    "    \n",
    "    # Save if specified\n",
    "    if filename is not None:\n",
    "        f.savefig(\"figures/{}_{}n_density.png\".format(filename, Y_test.shape[1]), dpi=600)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_true_est_scatter(model, reg, n_samples, X_test, theta_test, \n",
    "                          params_names, figsize=(20, 4), filename=None):\n",
    "    \"\"\"Plots a scatter plot with abline of the estimated posterior means vs true values.\"\"\"\n",
    "    \n",
    "    # Initialize figure\n",
    "    f, axarr = plt.subplots(1, len(params_names), figsize=figsize)\n",
    "\n",
    "    \n",
    "    # --- Sample parameter from approx posterior and calculate means and vars --- #\n",
    "    theta_samples = model.sample(X_test, n_samples).numpy()\n",
    "    theta_samples_means = theta_samples.mean(axis=0)\n",
    "        \n",
    "    # --- Compute analytic posterior and extract mean--- #\n",
    "    theta_true_means = np.zeros_like(theta_test.numpy())\n",
    "    for i in range(X_test.shape[0]): \n",
    "        post = reg.posterior_beta(X_test[i].numpy())\n",
    "        theta_true_means[i, :] = post.mean\n",
    "\n",
    "    # --- Plot true vs estimated posterior means on a single row --- #\n",
    "    for j in range(len(params_names)):\n",
    "        \n",
    "        # Plot analytic vs estimated\n",
    "        axarr[j].scatter(theta_samples_means[:, j], theta_true_means[:, j], color='black', alpha=0.4)\n",
    "        \n",
    "        # get axis limits and set equal x and y limits\n",
    "        lower_lim = min(axarr[j].get_xlim()[0], axarr[j].get_ylim()[0])\n",
    "        upper_lim = max(axarr[j].get_xlim()[1], axarr[j].get_ylim()[1])\n",
    "        axarr[j].set_xlim((lower_lim, upper_lim))\n",
    "        axarr[j].set_ylim((lower_lim, upper_lim))\n",
    "        axarr[j].plot(axarr[j].get_xlim(), axarr[j].get_xlim(), '--', color='black')\n",
    "        \n",
    "        # Compute NRMSD\n",
    "        rmsd = np.sqrt(np.mean( (theta_samples_means[:, j] - theta_true_means[:, j])**2 ))\n",
    "        nrmsd = rmsd / (theta_true_means[:, j].max() - theta_true_means[:, j].min())\n",
    "        axarr[j].text(0.2, 0.9, 'NRMSD={:.3f}'.format(nrmsd),\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center',\n",
    "                     transform=axarr[j].transAxes)\n",
    "        \n",
    "        # Compute R2\n",
    "        r2 = r2_score(theta_true_means[:, j], theta_samples_means[:, j])\n",
    "        axarr[j].text(0.2, 0.8, '$R^2$={:.3f}'.format(r2),\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center',\n",
    "                     transform=axarr[j].transAxes)\n",
    "        \n",
    "        if j == 0:\n",
    "            # Label plot\n",
    "            axarr[j].set_xlabel('Estimated Mean')\n",
    "            axarr[j].set_ylabel('Analytic Mean')\n",
    "        axarr[j].set_title(params_names[j])\n",
    "        axarr[j].spines['right'].set_visible(False)\n",
    "        axarr[j].spines['top'].set_visible(False)\n",
    "    \n",
    "    # Adjust spaces\n",
    "    f.tight_layout()\n",
    "    \n",
    "    # Save if specified\n",
    "    if filename is not None:\n",
    "        f.savefig(\"figures/{}_{}n_scatter.png\".format(filename, Y_test.shape[1]), dpi=600)\n",
    "        \n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def kl_mv_gaussian(mu_p, sigma_p, mu_q, sigma_q):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between two random MVNs.\n",
    "    Args:\n",
    "    mu_p    - vector of shape (n_dim, 1)\n",
    "    sigma_p - SPD matrix of shape (n_dim, n_dim)\n",
    "    mu_q    - vector of shape (n_dim, 1)\n",
    "    sigma_q - SPD matrix of shape (n_dim, n_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    d = sigma_p.shape[0]\n",
    "    log_det_sigma_q_p = np.log(np.linalg.det(sigma_q) / np.linalg.det(sigma_p))\n",
    "    tr_sigma_q_p = np.trace(np.linalg.inv(sigma_q) @ sigma_p)\n",
    "    mah_dist = (mu_q - mu_p) @ np.linalg.inv(sigma_q) @ (mu_q - mu_p)\n",
    "    kl = 0.5 * (log_det_sigma_q_p + tr_sigma_q_p + mah_dist - d)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Simulation Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# --- Structure of the coupling blocks in the INN chain --- #\n",
    "inv_meta = {\n",
    "    'n_units': [64, 64, 64],\n",
    "    'activation': 'elu',\n",
    "    'w_decay': 0.00001,\n",
    "    'initializer': 'glorot_uniform'\n",
    "}\n",
    "\n",
    "# --- Training hyperparameters --- #\n",
    "params_names = [r'$\\beta_{1}$', r'$\\beta_{2}$', r'$\\beta_{3}$', r'$\\beta_{4}$']\n",
    "theta_dim = 4\n",
    "global_step = tfe.Variable(0, dtype=tf.int32)\n",
    "batch_size = 64\n",
    "summary_dim = 64\n",
    "epochs = 100\n",
    "iterations_per_epoch = 1000\n",
    "n_inv_blocks = 10\n",
    "n_test = 500\n",
    "n_test2 = 3\n",
    "n_samples_posterior = 5000\n",
    "verbose_each = 100\n",
    "starter_learning_rate = 0.001\n",
    "decay_steps = 2000\n",
    "decay_rate = .9\n",
    "clip_value = 5.\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, decay_steps, decay_rate)\n",
    "\n",
    "\n",
    "# ----- Data generation function----- \n",
    "bayesian_regression = BayesianLinearRegression(theta_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "summary_net = PermutationInvariantNetwork(summary_dim)\n",
    "model = DeepConditionalModel(inv_meta, n_inv_blocks, theta_dim, summary_net=summary_net, permute=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, theta_test = bayesian_regression(n_test)\n",
    "X_test2, theta_test2 = bayesian_regression(n_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_posterior(model, bayesian_regression, n_samples_posterior, \n",
    "                        theta_test2, X_test2, params_names, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_scatter(model, bayesian_regression, n_samples_posterior, X_test, theta_test, params_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40000 iterations\n",
    "<p> The model thus sees batch_size * iterations data points. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for ep in range(1, epochs+1):\n",
    "    with tqdm(total=iterations_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        losses = train_loop_active(model, optimizer, bayesian_regression, iterations_per_epoch, \n",
    "                            batch_size, iterations_per_epoch, p_bar, clip_value=clip_value, \n",
    "                            global_step=global_step, ckpt_name='Bayesian_Regression_{}'.format(ep))\n",
    "        plot_losses(losses, figsize=(10, 4))\n",
    "        plot_true_est_posterior(model, bayesian_regression, n_samples_posterior, \n",
    "                        theta_test2, X_test2, params_names, figsize=(12, 4))\n",
    "        plot_true_est_scatter(model, bayesian_regression, n_samples_posterior, X_test, theta_test, params_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_est_posterior(model, bayesian_regression, 5000, \n",
    "                        theta_test2, X_test2, params_names, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_est_scatter(model, bayesian_regression, n_samples_posterior, X_test, theta_test, params_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/bayesian_regression_rnn_40k_final_decay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot approximate posteriors vs analytic posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, theta_test = bayesian_regression(n_test, n=50)\n",
    "X_test2, theta_test2 = bayesian_regression(n_test2, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_posterior(model, bayesian_regression, 5000, \n",
    "                        theta_test2, X_test2, params_names, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_scatter(model, bayesian_regression, n_samples_posterior, X_test, theta_test, params_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, theta_test = bayesian_regression(n_test, n=50)\n",
    "X_test2, theta_test2 = bayesian_regression(10, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_posterior(model, bayesian_regression, 5000, \n",
    "                        theta_test2, X_test2, params_names, figsize=(15, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_scatter(model, bayesian_regression, n_samples_posterior, X_test, theta_test, params_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = b_reg.generate_multiple_datasets(n_samples_test_desnity, to_tensor=True, n=250)\n",
    "X_test2, Y_test2 = b_reg.generate_multiple_datasets(n_samples_test_means, to_tensor=True, n=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_posterior(model, b_reg, 5000, X_test, Y_test, params_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_scatter(model, b_reg, 5000, X_test2, Y_test2, params_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = b_reg.generate_multiple_datasets(n_samples_test_desnity, to_tensor=True, n=500)\n",
    "X_test2, Y_test2 = b_reg.generate_multiple_datasets(n_samples_test_means, to_tensor=True, n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_posterior(model, b_reg, 5000, X_test, Y_test, params_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_scatter(model, b_reg, 5000, X_test2, Y_test2, params_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/bayesian_regression_rnn_40k_final_decay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = b_reg.generate_multiple_datasets(3, to_tensor=True, n=100)\n",
    "X_test2, Y_test2 = b_reg.generate_multiple_datasets(n_samples_test_means, to_tensor=True, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_posterior(model, b_reg, 5000, X_test, Y_test, params_names, figsize=(15, 5), filename='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_true_est_scatter(model, b_reg, 5000, X_test2, Y_test2, params_names, filename='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mmds = mmd_plot(model, b_reg, 500, X_test2, Y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "190.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
