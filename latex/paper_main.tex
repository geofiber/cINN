\documentclass[9pt,twoside,lineno]{pnas-new}
% Use the lineno option to display guide line numbers if required.

\templatetype{pnasmathematics} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission


\graphicspath{{figures/}}

\title{Learning complex models with invertible neural networks: a likelihood-free Bayesian approach}

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[1]{Stefan T. Radev}
\author[1]{Ulf K. Mertens} 
\author[1]{Andreas Voss}
\author[2]{Lynton Ardizzone}
\author[2]{Ullrich Köthe}

\affil[1]{Institute of Psychology, Heidelberg University, Hauptstr. 47-51, 69117 Heidelberg, Germany}
\affil[2]{Heidelberg Collaboratory for Image Processing (HCI), Interdisciplinary Center for Scientific Computing (IWR), Heidelberg University, Im Neuenheimer Feld 205, 69120 Heidelberg, Germany}

% Please give the surname of the lead author for the running footer
\leadauthor{Radev} 

% Please add here a significance statement to explain the relevance of your work
\significancestatement{Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the Significance Statement is to explain the relevance of the work in broad context to a broad readership. The Significance Statement appears in the paper itself and is required for all research papers.}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{This research was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation; grant number GRK 2277 "Statistical Modeling in Psychology")}


\correspondingauthor{\textsuperscript{2}To whom correspondence should be addressed. E-mail: stefan.radev\@psychologie.uni-heidelberg.de}

% At least three keywords are required at submission. Please provide two to five keywords, separated by the pipe symbol.
\keywords{Deep learning $|$ Invertible networks $|$ Bayesian inference $|$ Parameter estimation $|$ Stochastic models} 

\begin{abstract}
Parametric models of complex processes are ubiquitous throughout the sciences. As the processes under study and the models describing them become increasingly complex, parameter estimation with standard Bayesian and frequentist methods can quickly become intractable. To address this, we propose a novel method for likelihood-free inference based on invertible neural networks. The method is capable of performing fast full Bayesian inference on large amounts of data by training the networks on simulated data and learning to invert the model under study. The method is independent of particular data formats, as it includes a summary network trained to embed the observed data into fixed-size vectors in a data-driven way. This makes the method applicable to various scenarios where standard inference techniques fail. We demonstrate the utility of the method on a toy model with known analytic posterior and on example models from population dynamics, epidemiology, cognitive science and genetics. We argue for a general framework for building reusable parameter estimation machines for potentially any process model from which simulations can be obtained.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\dropcap{M}athematical models are formal descriptions of scientific theories allowing a clear and unambiguous way to formulate and test scientific hypotheses about probabilistic phenomena in a probabilistic world. In their most abstract form, mathematical models are specified by a set of latent parameters $\boldsymbol{\theta}$ and a generative model $q$ mapping parameters to manifest quantities $\boldsymbol{x}$: 

\begin{align*}
\boldsymbol{x}= q(\boldsymbol{\theta}) \numberthis \label{eqn:1} 
\end{align*}

While $q$ can represent an arbitrarily complex process by an arbitrarily complicated expression, its functional form is usually guided by a well-founded theoretical framework. For instance, it could be a stochastic differential equation describing the dynamics of single neurons in the brain, or a step-by-step mechanism dictating the rate of gene expression in certain cells. Thus, it is only through theoretical embedding that a meaningful interpretation in terms of some mechanism can be attached to the parameters of a mathematical model. Examples of mathematical models can be found in various scientific domains, e.g., genetics \cite{zappia2017splatter, beaumont2002approximate}, cognitive science \cite{palestro2018likelihood, usher2001time}, neuroscience \cite{hwang2018conditional, lueckmann2017flexible}, population dynamics \cite{wood2010statistical}, epidemiology \cite{hethcote2000mathematics}, just to name a few.

Once a mathematical model has been formulated, the next step consists of fitting the model to experimental or observational data and recovering the parameters of interest. However, estimating the parameters of a mathematical model can quickly become one of the most tenacious challenges in applications to real-world problems. It is also one of the most important ones to be tackled, since without reliable parameter estimation methods, it is impossible to test the utility of a model, regardless of its sophistication or theoretical appeal. Idealized parameter estimation involves computing the inverse (backward) model $\boldsymbol{\theta} = q^{-1}(\boldsymbol{x})$ exactly. However, due to noise, inherent stochasticity or loss of information, the inverse usually does not exist, so researchers need to resort to the sophisticated frameworks of Bayesian or frequentist inference. 

Moreover, as mathematical models and processes under description become increasingly complex, parameter estimation and model selection can quickly become intractable with standard Bayesian and frequentist method. Complex models specified by a generative stochastic mechanism do not always provide a closed-form solution for the \textit{likelihood function} \cite{palestro2018likelihood, csillery2010approximate, toni2009simulation}. This poses great difficulties for Bayesian and frequentist methods alike, since both depend explicitly on the numerical evaluation of a likelihood function as a proxy for assessing model fit to data. Even if a likelihood function is available, inference could be prohibitively slow for real-world applications. Therefore, the need for powerful and reliable likelihood-free estimation methods arises naturally. 

Likelihood-free methods aim at bypassing the intractability problem by resorting to a simulation-based approach to inference and model selection \cite{palestro2018likelihood}. A subset of likelihood-free methods includes approximate Bayesian computation (ABC) methods, which aim at preserving the advantages of Bayesian data analysis even when the likelihood function is intractable or practically impossible to compute \cite{turner2014generalized, sunnaaker2013approximate}. ABC methods approximate the likelihood function by repeatedly sampling parameters from a pre-specified prior distribution $\pi(\boldsymbol{\theta})$ and then simulating multiple datasets by running the generative model $q(\boldsymbol{\theta})$ using the sampled parameters. Thus, the core ingredients of ABC methods are a prior on $\boldsymbol{\theta}$, and a generative model $q(\boldsymbol{\theta})$, usually specified as a function code in a general-purpose programming language \cite{csillery2010approximate, mertens2018abrox}.

Performing approximate inference comes at the cost of incurring additional approximation error, which accumulates on top of the irreducible estimation error. Within the context of approximate inference, the most common manifestations of approximation error include: \textit{i)} imprecise form of the posterior; \textit{ii)} imprecise posterior moments; \textit{iii)} under- or overestimation of uncertainty. Different approximation methods usually involve multiple trade-offs between minimizing approximation error and keeping computational time within reasonable bounds \cite{frazier2018asymptotic, palestro2018likelihood}.

Recently, ideas from machine learning and deep learning research have entered the field of likelihood-free inference in an attempt to overcome some of the shortcomings of traditional methods \cite{radev2019towards, hwang2018conditional, mestdagh2018prepaid, raynal2018abc, jiang2017learning, lueckmann2017flexible, papamakarios2016fast}. The most common approach has been to cast the problem of parameter estimation as a supervised learning task. In this setting, a large dataset of the form $\boldsymbol{D} = \{h(\boldsymbol{x}^{(i)}), \boldsymbol{\theta}^{(i)}\}_{i=1}^{n}$ is created by repeatedly sampling from $\pi(\boldsymbol{\theta})$ and simulating an artificial dataset $\boldsymbol{x}$ by running $q(\boldsymbol{\theta})$ with the sampled parameters. Usually, the dimensionality of the simulated data is reduced by computing summary statistics  with a fixed summary function $h(\boldsymbol{x})$. Then, a supervised learning algorithm $f(h(\boldsymbol{x});\boldsymbol{\phi}) = \widehat{\boldsymbol{\theta}}$ with learnable parameters $\boldsymbol{\phi}$ (e.g., linear regression, random forest, neural network) is trained on the simulated data to later output an estimate of the true data generating parameters. Thus, $f(h(\boldsymbol{x});\boldsymbol{\phi})$ essentially attempts to “learn” the intractable inverse model $\boldsymbol{\theta} = q^{-1}(\boldsymbol{x})$.

Inspired by previous machine learning approaches, the current work proposes a novel and universal likelihood-free method capable of performing full Bayesian inference on any mathematical process model from which simulations can be obtained. It treats parameter inference as a task of inverting a generative model and achieves this by drawing on the modern framework of deep probabilistic modeling for tackling intractable posteriors \cite{ardizzone2018analyzing, kingma2018glow, grover2018flow, dinh2016density}. The method integrates two separate deep neural networks modules (detailed in the \textbf{Methods} section) trained jointly on simulated data: a \textit{summary network} and an \textit{invertible network}. 

The \textit{summary network} is responsible for learning the most informative summary statistics directly from data. It should be designed to follow the functional and probabilistic symmetries inherent in the data, e.g. a permutationally invariant network for \textit{i.i.d.} data \cite{bloem2019probabilistic}, a recurrent network for time-series data, or a convolutional network for grid-like data \cite{goodfellow2016deep}. The computation of summary statistics is a crucial aspect in likelihood-free inference. Previous approaches mainly use hand-crafted summary statistics tailored to the specific application. However, in many application, it is not straightforward to settle upon a set of good summary statistics. Thus, the summary network completely eliminates the need to manually specify a fixed number of summary statistics and makes the method independent of the format or the size of the data.

The \textit{invertible network} is responsible for learning the posterior of the model parameters given the observed and summarized data. It is based on the recently developed flow-based architecture \cite{kingma2018glow, grover2018flow, dinh2016density}. Flow-based methods provide exact latent-variable inference and log-likelihood evaluation when operating at optimum. In the \textbf{Methods} section, we show that our method maximizes the posterior over model parameters directly when cast in the context of likelihood-free inference. Furthermore, flow-based methods are capable of approximating very high-dimensional distributions (e.g., images). Once trained with a sufficient amount of simulated data, our invertible network can perform full Bayesian inference on large amounts of real data from a given domain in a single pass. 

The joint training of a summary network and an invertible network results in a powerful and universal parameter estimation machine capable of inverting complicated statistical problems in various scientific domains (see Fig.1). Moreover, the method addresses many of the limitations of previous likelihood-free methods. First, it involves no costly MCMC or rejection sampling, which makes the inference phase lightning fast, as we also make use of GPU-accelerated computation. Second, it involves no fixed summary statistics or kernels, but learns the most informative representation of the data in an end-to-end manner. Third, the method is fully Bayesian, as it directly learns the posterior over model parameters and thus allows for the quantification of uncertainty, which is a crucial requirement in parameter estimation \cite{kendall2017uncertainties, gelman2013bayesian}. Last, the trained networks can be shared and reused by multiple researches within a scientific domain, thus removing the need for wasteful computations and fitting a separate model for each and every dataset. This pooling of computational resources across researches is an important step forward in mathematical modeling, as has been recently argued \cite{mestdagh2018prepaid}.

To illustrate the utility of the new method, we first apply it to a toy Bayesian regression model with known posterior. Then, we present applications to intractable models from cognitive science, population dynamics, epidemiology, and genetics and demonstrate state-of-the art parameter recovery. The outline of the remaining manuscript is as follows: The \textbf{Methods} section introduces the main building blocks of the new method and summarizes the main steps as pseudocode. The \textbf{Results} section presents the various applications of the model to real-world research domains. Finally, the \textbf{Discussion} section lists the advantages of the current method, treats some potential pitfalls and explores future research vistas. Python code and data for all applications are freely available as Jupyter notebooks at \href{https://github.com/stefanradev93/cINN}{https://github.com/stefanradev93/cINN} and as a small library based on \textit{TensorFlow} \cite{abadi2016tensorflow} for creating and training custom invertible networks with GPU-support. 


\section*{Methods}


\subsection*{Notation}

In the following, we denote observed or simulated univariate datasets from the mathematical model of interest as $\boldsymbol{x} = (x_{1}, x_{2},...,x_{n})$, and multivariate datasets as $\boldsymbol{X} = (\boldsymbol{x}_{1}, \boldsymbol{x}_{1},...,\boldsymbol{x}_{n})$. The parameters of a mathematical model are represented as a vector $\boldsymbol{\theta} = (\theta_{1}, \theta_{2},...,\theta_{d})$, and all trainable parameters of the invertible and summary neural networks as $\boldsymbol{\phi} = (\boldsymbol{\phi}_{inv}, \boldsymbol{\phi}_{sum})$. The number of parameters of a mathematical model will be denoted as $d$, and the number of simulated data points as $n$.

\subsection*{Deep Probabilistic Modeling}

Our method draws on major advances in modern deep probabilistic modeling, also referred to as deep generative modeling \cite{bloem2019probabilistic, kingma2018glow, ardizzone2018analyzing, kingma2014auto}. A hallmark idea in deep probabilistic modeling is to handle intractable target probability distributions by sampling from simpler distributions (e.g., Gaussian or uniform distributions) and transforming these samples via a complex non-linear, learnable transformations. Most popular deep probabilistic models entail two phases. During the \textit{training phase}, a transformation from the simple to the desired target distribution is learned by optimizing a cost function via backpropagation. During the \textit{inference phase}, samples from the target distribution are obtained by sampling from the simple distribution and applying the transformation learned during the training phase. Using this approach, recent applications of deep probabilistic models have achieved unprecedented results on extremely high-dimensional and intractable problems (e.g., complex data distributions such as natural images, music, or text).

In the context of mathematical modeling and Bayesian inference, the target distribution is the posterior distribution of model parameters $p(\boldsymbol{\theta}|\boldsymbol{x})$ capturing our uncertainty about the numerical values of parameters given empirical data. We can leverage the fact that most mathematical models are generative in nature and as such can be used to perform multiple simulations of the process of interest. By specifying a prior distribution over the model parameters $\pi(\boldsymbol{\theta})$, one can generate arbitrarily large datasets of the form $\boldsymbol{D} = \{\boldsymbol{x}^{(i)}, \boldsymbol{\theta}^{(i)}\}_{i=1}^{n}$ and use a deep generative model to learn a probabilistic mapping from data to parameters. Thus, at inference time, one can condition the model on observed data $\boldsymbol{x}_{obs}$ and obtain samples $\boldsymbol{\theta}_{1}, \boldsymbol{\theta}_{2},...,\boldsymbol{\theta}_{L}$  approximating the posterior $p(\boldsymbol{\theta}|\boldsymbol{x}=\boldsymbol{x}_{obs})$ in the manner described above.

In the current work, we propose to implement and use a conditional invertible neural network (cINN) architecture. Previously, INNs have been successfully employed to model data from astrophysics and medicine\cite{ardizzone2018analyzing}. We adapt the model to suit the task of parameter estimation in the context of mathematical modeling (see Figure 1 for a full graphical illustration of the method) and develop a reusable probabilistic architecture for full Bayesian likelihood-free inference on complex mathematical models.

\subsection*{The Affine Coupling Block}

The basic building block of a cINN is the affine coupling block (ACB) \cite{ardizzone2018analyzing, kingma2018glow, dinh2016density}. Each ACB consists of four separate fully connected neural networks denoted as $s_{1}(\cdot), s_{2}(\cdot), t_{1}(\cdot), t_{2}(\cdot)$. An ACB is specifically designed to be invertible, which means that in addition to a parametric mapping $f_{\boldsymbol{\phi}_{inv}}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ is also learns the inverse mapping $f_{\boldsymbol{\phi}_{inv}}^{-1}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ "for free". Denoting the input vector of $f_{\boldsymbol{\phi}_{inv}}$ as $\boldsymbol{u}$ and the output vector as $\boldsymbol{v}$, it follows that $f(\boldsymbol{u}; \boldsymbol{\phi}_{inv}) = \boldsymbol{v}$ and $f^{-1}(\boldsymbol{v}; \boldsymbol{\phi}_{inv}) = \boldsymbol{u}$. Invertibility is achieved by splitting the input vector into two parts $\boldsymbol{u} = (\boldsymbol{u}_{1}, \boldsymbol{u}_{2})$ and performing the following operations on the split input:
\begin{align*} 
\boldsymbol{v}_{1} &= \boldsymbol{u}_{1} \odot exp(s_{1}(\boldsymbol{u}_{2})) + t_{1}(\boldsymbol{u}_{2}) \numberthis \label{eqn:2}  \\ 
\boldsymbol{v}_{2} &= \boldsymbol{u}_{2} \odot exp(s_{1}(\boldsymbol{v}_{1})) + t_{1}(\boldsymbol{v}_{1}) \numberthis \label{eqn:3} 
\end{align*}
The outputs $\boldsymbol{v} = (\boldsymbol{v}_{1}, \boldsymbol{v}_{2})$ are then concatenated again and passed to the next ACB. The inverse operation is given by:
\begin{align*} 
\boldsymbol{u}_{2} &= (\boldsymbol{v}_{2} - t_{2}(\boldsymbol{v}_{1})) \odot exp(-s_{2}(\boldsymbol{v}_{1})) \numberthis \label{eqn:4}  \\ 
\boldsymbol{u}_{1} &= (\boldsymbol{v}_{1} - t_{1}(\boldsymbol{u}_{2})) \odot exp(-s_{1}(\boldsymbol{u}_{2})) \numberthis \label{eqn:5} 
\end{align*}
An additional property of this design, which becomes relevant later for optimization, is that the operations of the ACB have tractable, and cheaply computable Jacobians (strictly upper or lower triangular matrices). Furthermore, the internal networks $s_{1}(\cdot), s_{2}(\cdot), t_{1}(\cdot), t_{2}(\cdot)$ can be represented by arbitrarily complex neural networks, which themselves need not be invertible, since they are only ever evaluated in the forward direction during both the forward and the inverse pass through the ACB. To ensure that the model is powerful enough to represent complicated distributions, we chain multiple ACBs, so that the output of each ACB becomes the input of the next. In this way, the whole chain remains invertible from the first input to the last output and can be viewed as a single function parameterized by trainable parameters $\boldsymbol{\phi}_{inv}$.

In our applications, the input to the first ACB is the parameter vector $\boldsymbol{\theta}$, and the output of the final ACB, denoted hitherto as $\boldsymbol{z}$, is encouraged to follow a $d$-dimensional spherical Gaussian via optimization (described in detail later), that is, $p(\boldsymbol{z}) = \mathcal{N}_{d}(\boldsymbol{z}|\boldsymbol{0},\boldsymbol{I})$. Fixed permutation matrices are used before each ACB to ensure that each axis of the latent space encodes information from all components of $\boldsymbol{\theta}$. In order to take into account the observed data $\boldsymbol{x}$, each of the internal networks of each ACB is augmented to take $\boldsymbol{x}$ as an additional input - $s_{1}(\cdot,\boldsymbol{x}), s_{2}(\cdot,\boldsymbol{x}), t_{1}(\cdot,\boldsymbol{x}), t_{2}(\cdot,\boldsymbol{x})$ - so a complete pass through the entire invertible chain can be expressed as:
\begin{align*} 
f(\boldsymbol{\theta};\boldsymbol{x},\boldsymbol{\phi}_{inv}) = \boldsymbol{z} \numberthis \label{eqn:6}
\end{align*}
together with the inverse operation:
\begin{align*} 
f^{-1}(\boldsymbol{z};\boldsymbol{x},\boldsymbol{\phi}_{inv}) = \boldsymbol{\theta} \numberthis \label{eqn:7}
\end{align*}

This process can be interpreted as follows: the forward pass maps data-generating parameters to $\boldsymbol{z}$-space using conditional information of $\boldsymbol{x}$, while the inverse pass maps data points from $\boldsymbol{z}$-space to the data-generating parameters of interest using the same conditional information provided by the data. In the next section, we describe the optimization procedure used to match the outputs of $f^{-1}(\boldsymbol{z};\boldsymbol{x},\boldsymbol{\phi}_{inv})$ to the posterior $p(\boldsymbol{\theta}|\boldsymbol{x})$.

\subsection*{Summary Network}
Since in practice the conditioning data set $\boldsymbol{x}$ can have variable number of input points (e.g., trial sizes, time points) and exhibit various redundancies, the cINN can profit from some form of dimensionality reduction applied to the data. Ideally, we want to avoid hand-crafted summary statistics, and instead learn the most informative summary statistics directly from data. Therefore, instead of feeding the raw simulated (observed) data to each ACB, we pass the data through an additional summary network to obtain a fixed-sized vector of learned summary statistics $\tilde{\boldsymbol{x}} = h(\boldsymbol{x};\boldsymbol{\phi}_{sum})$ and learn the parameters of the summary network $h$ jointly with those of the cINN chain via backpropagation. Thus, the current method remains completely end-to-end and is capable of generalizing to data sets of variable input size and structure.

\subsection*{Maximum Likelihood Loss}
Broadly speaking, the goal of maximum likelihood estimation is to find a set of parameters which maximize the probability of the data under a parametric model. In our case, we are interested in maximizing the expectation over possible neural network parameters with respect to the parameters of the mathematical model:


\subsection*{Submitting Manuscripts}

All authors must submit their articles at \href{http://www.pnascentral.org/cgi-bin/main.plex}{PNAScentral}. If you are using Overleaf to write your article, you can use the ``Submit to PNAS'' option in the top bar of the editor window. 

\subsection*{Format}

Many authors find it useful to organize their manuscripts with the following order of sections;  Title, Author Affiliation, Keywords, Abstract, Significance Statement, Results, Discussion, Materials and methods, Acknowledgments, and References. Other orders and headings are permitted.

\subsection*{Manuscript Length}

PNAS generally uses a two-column format averaging 67 characters, including spaces, per line. The maximum length of a Direct Submission research article is six pages and a Direct Submission Plus research article is ten pages including all text, spaces, and the number of characters displaced by figures, tables, and equations.  When submitting tables, figures, and/or equations in addition to text, keep the text for your manuscript under 39,000 characters (including spaces) for Direct Submissions and 72,000 characters (including spaces) for Direct Submission Plus.


\subsection*{Data Archival}

PNAS must be able to archive the data essential to a published article. Where such archiving is not possible, deposition of data in public databases, such as GenBank, ArrayExpress, Protein Data Bank, Unidata, and others outlined in the Information for Authors, is acceptable.

\subsection*{Language-Editing Services}
Prior to submission, authors who believe their manuscripts would benefit from professional editing are encouraged to use a language-editing service (see list at www.pnas.org/site/authors/language-editing.xhtml). PNAS does not take responsibility for or endorse these services, and their use has no bearing on acceptance of a manuscript for publication. 

\begin{figure}%[tbhp]
\centering
\includegraphics[scale=0.1]{figure1.png}
\caption{Placeholder image of a frog with a long example caption to show justification setting.}
\label{fig:frog}
\end{figure}


\subsection*{Digital Figures}

Only TIFF, EPS, and high-resolution PDF for Mac or PC are allowed for figures that will appear in the main text, and images must be final size. Authors may submit U3D or PRC files for 3D images; these must be accompanied by 2D representations in TIFF, EPS, or high-resolution PDF format.  Color images must be in RGB (red, green, blue) mode. Include the font files for any text. 

Figures and Tables should be labelled and referenced in the standard way using the \verb|\label{}| and \verb|\ref{}| commands.

Figure \ref{fig:frog} shows an example of how to insert a column-wide figure. To insert a figure wider than one column, please use the \verb|\begin{figure*}...\end{figure*}| environment. Figures wider than one column should be sized to 11.4 cm or 17.8 cm wide. Use \verb|\begin{SCfigure*}...\end{SCfigure*}| for a wide figure with side captions.


\subsection*{Single column equations}

Authors may use 1- or 2-column equations in their article, according to their preference.

To allow an equation to span both columns, use the \verb|\begin{figure*}...\end{figure*}| environment mentioned above for figures.



\subsection*{Supporting Information Appendix (SI)}

Authors should submit SI as a single separate PDF file, combining all text, figures, tables, movie legends, and SI references.  PNAS will publish SI uncomposed, as the authors have provided it.  Additional details can be found here: \href{https://www.pnas.org/page/authors/submission}{policy on SI}.  For SI formatting instructions click \href{https://www.pnascentral.org/cgi-bin/main.plex?form_type=display_auth_si_instructions}{here}.  The PNAS Overleaf SI template can be found \href{https://www.overleaf.com/latex/templates/pnas-template-for-supplementary-information/wqfsfqwyjtsd}{here}.  Refer to the SI Appendix in the manuscript at an appropriate point in the text. Number supporting figures and tables starting with S1, S2, etc.

Authors who place detailed materials and methods in an SI Appendix must provide sufficient detail in the main text methods to enable a reader to follow the logic of the procedures and results and also must reference the SI methods. If a paper is fundamentally a study of a new method or technique, then the methods must be described completely in the main text.

\subsubsection*{SI Datasets} 

Supply .xlsx, .csv, .txt, .rtf, or .pdf files. This file type will be published in raw format and will not be edited or composed.



\acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}

\showacknow % Display the acknowledgements section

% Bibliography
\bibliography{references}

\end{document}